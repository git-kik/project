# -*- coding: utf-8 -*-
"""xray_detection1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bQ4-rQrWhb1PMZKzlvWqiRGumeRqskCh
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Define paths to your data folders
train_dir = '/content/drive/MyDrive/project/new-images/train'
test_dir = '/content/drive/MyDrive/project/new-images/test'

# Get the class names names
import os
class_names = np.array(sorted(os.listdir(train_dir)))
print(class_names)

img_width, img_height = 224, 224
batch_size = 32

# Rescale the data and create data generator instances
train_datagen = ImageDataGenerator(rescale=1/255.)
test_datagen = ImageDataGenerator(rescale=1/255.)

# Load data in from directories and turn it into batches
train_data = train_datagen.flow_from_directory(train_dir,
                                               target_size=(224, 224),
                                               batch_size=batch_size,
                                               class_mode='categorical') # changed to categorical

test_data = train_datagen.flow_from_directory(test_dir,
                                              target_size=(224, 224),
                                              batch_size=batch_size,
                                              class_mode='categorical')

images, labels = train_data.next() # get the 'next' batch of images/labels
len(images), len(labels)

labels

# Plot the validation and training data separately
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

def model_save(model,model_num):
  model.save(f'/content/drive/MyDrive/project/models/xray_classification_{model_num}_new.keras')

# Create ImageDataGenerator training instance with data augmentation
train_datagen_augmented = ImageDataGenerator(rescale=1/255.,
                                             rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)
                                             shear_range=0.2, # shear the image
                                             zoom_range=0.2, # zoom into the image
                                             width_shift_range=0.2, # shift the image width ways
                                             height_shift_range=0.2, # shift the image height ways
                                             horizontal_flip=True) # flip the image on the horizontal axis

# Create ImageDataGenerator training instance without data augmentation
train_datagen = ImageDataGenerator(rescale=1/255.)

# Create ImageDataGenerator test instance without data augmentation
test_datagen = ImageDataGenerator(rescale=1/255.)

# Import data and augment it from training directory
print("Augmented training images:")
train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,
                                                                   target_size=(224, 224),
                                                                   batch_size=32,
                                                                   class_mode='categorical',
                                                                   shuffle=False) # Don't shuffle for demonstration purposes, usually a good thing to shuffle

# Create non-augmented data batches
print("Non-augmented training images:")
train_data = train_datagen.flow_from_directory(train_dir,
                                               target_size=(224, 224),
                                               batch_size=32,
                                               class_mode='categorical',
                                               shuffle=False) # Don't shuffle for demonstration purposes

print("Unchanged test images:")
test_data = test_datagen.flow_from_directory(test_dir,
                                             target_size=(224, 224),
                                             batch_size=32,
                                             class_mode='categorical')

# Get data batch samples
augmented_images, augmented_labels = train_data_augmented.next()

import random
# Display the first original and augmented images
random_number = random.randint(0, 31)  # Randomly select an index from the batch
original_image, original_label = train_data[0][0][random_number], train_data[0][1][random_number]
augmented_image, augmented_label = augmented_images[random_number], augmented_labels[random_number]

# Print the labels
print("Original image label:", original_label)
print("Augmented image label:", augmented_label)

# Plot the original and augmented images
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.imshow(original_image)
plt.title("Original Image")
plt.axis(False)

plt.subplot(1, 2, 2)
plt.imshow(augmented_image)
plt.title("Augmented Image")
plt.axis(False)

plt.show()

from tensorflow.keras import layers

def create_base_model(input_shape = (224, 224, 3),
                      output_shape=7,
                      learning_rate = 0.001,
                      training = False):

    # Create base model
    base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
    base_model.trainable = training

    # Setup model input and outputs
    inputs = layers.Input(shape=input_shape, name="input_layer")
    x = base_model(inputs, training=False)  # pass augmented images to base model but keep it in inference mode
    x = layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)
    outputs = layers.Dense(units=output_shape, activation="softmax", name="output_layer")(x)
    model = tf.keras.Model(inputs, outputs)

    # Compile model
    model.compile(loss="categorical_crossentropy",
                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  metrics=["accuracy"])

    return model

model = create_base_model()

initial_epochs = 10
history = model.fit(train_data,
                        steps_per_epoch=train_data.samples // batch_size,
                        epochs=initial_epochs,
                        validation_data=test_data,
                        validation_steps=int(0.25 * len(test_data))
                        )

model_save(model,1)

plot_loss_curves(history)

# Layers in loaded model
model.layers

# Access the base_model layers of resnet
model_base_model = model.layers[1]
model_base_model.name

print(len(model_base_model.trainable_variables)) # layer at index 2 is the EfficientNetV2B0 layer (the base model)

# Check which layers are tuneable (trainable)
for layer_number, layer in enumerate(model_base_model.layers):
  print(layer_number, layer.name, layer.trainable)

def adjust_model_layers(model_base_model,
    model_layers,
    learning_rate=0.0001
    ):
  model_base_model.trainable = True

  # Freeze all layers except for the last 5
  for layer in model_base_model.layers[:-model_layers]:
    layer.trainable = False

  # Recompile the whole model (always recompile after any adjustments to a model)
  return model.compile(loss="categorical_crossentropy",
                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), # lr is 10x lower than before for fine-tuning
                  metrics=["accuracy"])

adjust_model_layers(model_base_model,5)

print(len(model_base_model.trainable_variables))

fine_tune_epochs = initial_epochs + 5

history_finetune = model.fit(train_data,
                              epochs=fine_tune_epochs,
                              validation_data=test_data,
                              initial_epoch=history.epoch[-1], # starting from previous last epoch
                              validation_steps=int(0.25 * len(test_data)))

model_save(model,2)

# Create a function to import an image and resize it to be able to be used with our model
def load_and_prep_image(filename, img_shape=224):
  """
  Reads an image from filename, turns it into a tensor
  and reshapes it to (img_shape, img_shape, colour_channel).
  """
  # Read in target file (an image)
  img = tf.io.read_file(filename)

  # Decode the read file into a tensor & ensure 3 colour channels
  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)
  img = tf.image.decode_image(img, channels=3)

  # Resize the image (to the same size our model was trained on)
  img = tf.image.resize(img, size = [img_shape, img_shape])

  # Rescale the image (get all values between 0 and 1)
  img = img/255.
  return img

# Adjust function to work with multi-class
def pred_and_plot(model, filename, class_names):
  """
  Imports an image located at filename, makes a prediction on it with
  a trained model and plots the image with the predicted class as the title.
  """
  # Import the target image and preprocess it
  img = load_and_prep_image(filename)

  # Make a prediction
  pred = model.predict(tf.expand_dims(img, axis=0))

  # Get the predicted class
  if len(pred[0]) > 1: # check for multi-class
    pred_class = class_names[pred.argmax()] # if more than one output, take the max
  else:
    pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round

  # Plot the image and predicted class
  plt.imshow(img)
  plt.title(f"Prediction: {pred_class}")
  plt.axis(False);

pred_and_plot(model, '/content/drive/MyDrive/project/x-rays-pneumonia/test_images/test_images/img_9013585571058568187.jpg', class_names)

def compare_historys(original_history, new_history, initial_epochs=5):

    # Get original history
    acc = original_history.history["accuracy"]
    loss = original_history.history["loss"]

    print(len(acc))

    val_acc = original_history.history["val_accuracy"]
    val_loss = original_history.history["val_loss"]

    # Combine original history with new history
    total_acc = acc + new_history.history["accuracy"]
    total_loss = loss + new_history.history["loss"]

    total_val_acc = val_acc + new_history.history["val_accuracy"]
    total_val_loss = val_loss + new_history.history["val_loss"]

    print(len(total_acc))
    print(total_acc)

    plt.figure(figsize=(8, 8))
    plt.subplot(2, 1, 1)
    plt.plot(total_acc, label='Training Accuracy')
    plt.plot(total_val_acc, label='Validation Accuracy')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    plt.subplot(2, 1, 2)
    plt.plot(total_loss, label='Training Loss')
    plt.plot(total_val_loss, label='Validation Loss')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.xlabel('epoch')
    plt.show()

compare_historys(original_history=history,
                 new_history=history_finetune,
                 initial_epochs=5)

adjust_model_layers(model_base_model,10)

# Continue to train and fine-tune the model to our data
fine_tune_epochs = initial_epochs + 5

history_finetune_10 = model.fit(train_data,
                                           epochs=fine_tune_epochs,
                                           initial_epoch=history_finetune.epoch[-1],
                                           validation_data=test_data,
                                           validation_steps=int(0.25 * len(test_data))
                                           )

compare_historys(original_history=history,
                 new_history=history_finetune_10,
                 initial_epochs=5)

model_save(model,3)

adjust_model_layers(model_base_model,20)

fine_tune_epochs = initial_epochs + 5

history_finetune_20 = model.fit(train_data,
                                           epochs=fine_tune_epochs,
                                           initial_epoch=history_finetune.epoch[-1],
                                           validation_data=test_data,
                                           validation_steps=int(0.25 * len(test_data))
                                           )

compare_historys(original_history=history,
                 new_history=history_finetune_20,
                 initial_epochs=5)

model_save(model,4)

pred_and_plot(model, '/content/drive/MyDrive/project/x-rays-pneumonia/test_images/test_images/img_9013585571058568187.jpg', class_names)

pred_and_plot(model, '/content/drive/MyDrive/project/new-images/test/human/rider-196.jpg', class_names)

pred_and_plot(model, '/content/drive/MyDrive/project/x-rays-pneumonia/test_images/test_images/img_9013585571058568187.jpg', class_names)

pred_and_plot(model, '/content/drive/MyDrive/project/new-images/test/chest-xray/img_180688085215986561.jpg', class_names)

pred_and_plot(model,'/content/drive/MyDrive/project/new-images/test/dog/dog.196.jpg',class_names)

file_path = os.path.join(test_dir, '/content/drive/MyDrive/project/images/test/flower/0081.png')

if os.path.isfile(file_path):
    print("File exists and is accessible.")
else:
    print("File does not exist or is not accessible.")

